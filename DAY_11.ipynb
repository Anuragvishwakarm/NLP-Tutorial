{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python – Lemmatization Approaches with Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a step by step guide to exploring various kinds of Lemmatization approaches in python along with a few examples and code implementation. It is highly recommended that you stick to the given flow unless you have an understanding of the topic, in which case you can look up any of the approaches given below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Lemmatization? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to 'stemming', lemmatization is a lot more powerful. It looks beyond word reduction and considers a language’s full vocabulary to apply a **morphological analysis** to words, aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the **lemma.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TIP: Always convert your text to lowercase before performing any NLP task including lemmatizing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Various Approaches to Lemmatization:** \n",
    "\n",
    "We will be going over **9 different approaches** to perform Lemmatization along with multiple examples and code implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. WordNet\n",
    "2. WordNet (with POS tag)\n",
    "3. TextBlob\n",
    "4. TextBlob (with POS tag)\n",
    "5. spaCy\n",
    "6. TreeTagger\n",
    "7. Pattern\n",
    "8. Gensim\n",
    "9. Stanford CoreNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Wordnet Lemmatizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordnet is a publicly available lexical database of over 200 languages that provides semantic relationships between its words. It is one of the earliest and most commonly used lemmatizer technique. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is present in the nltk library in python.\n",
    "- Wordnet links words into semantic relations. ( eg. synonyms )\n",
    "- It groups synonyms in the form of synsets.\n",
    "    - synsets : a group of data elements that are semantically equivalent. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to use:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download nltk package : In your anaconda prompt or terminal, type:\n",
    "\n",
    "**pip install nltk**\n",
    "\n",
    "2. Download Wordnet from nltk : In your python console, do the following :\n",
    "\n",
    "**import nltk** \n",
    "\n",
    "**nltk.download(‘wordnet’)** \n",
    "\n",
    "**nltk.download(‘averaged_perceptron_tagger’)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create WordNetLemmatizer object\n",
    "obj = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kites  -----> kite\n",
      "babies  -----> baby\n",
      "dogs  -----> dog\n",
      "flying  -----> flying\n",
      "smiling  -----> smiling\n",
      "driving  -----> driving\n",
      "died  -----> died\n",
      "tried  -----> tried\n",
      "feet  -----> foot\n"
     ]
    }
   ],
   "source": [
    "# single word lemmatization examples\n",
    "list1 = ['kites', 'babies', 'dogs', 'flying', 'smiling', \n",
    "         'driving', 'died', 'tried', 'feet']\n",
    "\n",
    "for words in list1:\n",
    "    print(words + \"  -----> \" +obj.lemmatize(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentence lemmatization examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cat', 'is', 'sitting', 'with', 'the', 'bats', 'on', 'the', 'striped', 'mat', 'under', 'many', 'flying', 'geese']\n",
      "the cat is sitting with the bat on the striped mat under many flying goose\n"
     ]
    }
   ],
   "source": [
    "# sentence lemmatization examples\n",
    "string = 'the cat is sitting with the bats on the striped mat under many flying geese'\n",
    "\n",
    "# Converting String into tokens\n",
    "list2 = nltk.word_tokenize(string)\n",
    "print(list2)\n",
    "#> ['the', 'cat', 'is', 'sitting', 'with', 'the', 'bats', 'on',\n",
    "# 'the', 'striped', 'mat', 'under', 'many', 'flying', 'geese']\n",
    "\n",
    "lemmatized_string = ' '.join([obj.lemmatize(words) for words in list2])\n",
    "\n",
    "print(lemmatized_string) \n",
    "#> the cat is sitting with the bat on the striped mat under many flying goose\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Wordnet Lemmatizer (with POS tag) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above approach, we observed that Wordnet results were not up to the mark. Words like ‘sitting’, ‘flying’ etc remained the same after lemmatization. This is because these words are treated as a noun in the given sentence rather than a verb. To overcome come this, we use POS (Part of Speech) tags. \n",
    "We add a tag with a particular word defining its type (verb, noun, adjective etc). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Example**\n",
    "\n",
    "| Word     | POS Tag (Type) | Lemmatized Word |\n",
    "|----------|----------------|-----------------|\n",
    "| driving  | verb ('v')      | drive           |\n",
    "| dogs     | noun ('n')      | dog             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# WORDNET LEMMATIZER (with appropriate pos tags)\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to lemmatize each word with its POS tag\n",
    "# POS_TAGGER_FUNCTION : TYPE 1\n",
    "\n",
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "sentence = 'the cat is sitting with the bats on the striped mat under many badly flying geese'\n",
    "# tokenize the sentence and find the POS tag for each token\n",
    "pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('cat', 'NN'), ('is', 'VBZ'), ('sitting', 'VBG'), ('with', 'IN'), ('the', 'DT'), ('bats', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('striped', 'JJ'), ('mat', 'NN'), ('under', 'IN'), ('many', 'JJ'), ('badly', 'RB'), ('flying', 'VBG'), ('geese', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', None), ('cat', 'n'), ('is', 'v'), ('sitting', 'v'), ('with', None), ('the', None), ('bats', 'n'), ('on', None), ('the', None), ('striped', 'a'), ('mat', 'n'), ('under', None), ('many', 'a'), ('badly', 'r'), ('flying', 'v'), ('geese', 'a')]\n"
     ]
    }
   ],
   "source": [
    "# we use our own pos_tagger function to make things simpler to understand.\n",
    "wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "print(wordnet_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cat be sit with the bat on the striped mat under many badly fly geese\n"
     ]
    }
   ],
   "source": [
    "lemmatized_sentence = []\n",
    "for word, tag in wordnet_tagged:\n",
    "    if tag is None:\n",
    "        # if there is no available tag, append the token as is\n",
    "        lemmatized_sentence.append(word)\n",
    "    else:        \n",
    "        # else use the tag to lemmatize the token\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "lemmatized_sentence = \" \".join(lemmatized_sentence)\n",
    " \n",
    "print(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. TextBlob "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextBlob is a python library used for processing textual data. It provides a simple API to access its methods and perform basic NLP tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download TextBlob package :**\n",
    " In your anaconda prompt or terminal, type: \n",
    " \n",
    "pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Word\n",
    "\n",
    "my_word='coolings'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a Word object\n",
    "w = Word(my_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cooling\n"
     ]
    }
   ],
   "source": [
    "print(w.lemmatize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'the bats saw the cats with stripes hanging upside down by their feet.'\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = TextBlob(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_sentence=' '.join([w.lemmatize() for w in S.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the bat saw the cat with stripe hanging upside down by their foot\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. TextBlob (with POS tag) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as in Wordnet approach without using appropriate POS tags, we observe the same limitations in this approach as well. So, we use one of the more powerful aspects of the TextBlob module the **‘Part of Speech’** tagging to overcome this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Define function to lemmatize each word with its POS tag\n",
    "\n",
    "# POS_TAGGER_FUNCTION : TYPE 2\n",
    "def pos_tagger(sentence):\n",
    "\tsent = TextBlob(sentence)\n",
    "\ttag_dict = {\"J\": 'a', \"N\": 'n', \"V\": 'v', \"R\": 'r'}\n",
    "\twords_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags] \n",
    "\tlemma_list = [wd.lemmatize(tag) for wd, tag in words_tags]\n",
    "\treturn lemma_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the bat saw the cat with stripe hang upside down by their foot\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize\n",
    "sentence = \"the bats saw the cats with stripes hanging upside down by their feet\"\n",
    "lemma_list = pos_tagger(sentence)\n",
    "lemmatized_sentence = \" \".join(lemma_list)\n",
    "print(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the bat saw the cat with stripe hanging upside down by their foot\n"
     ]
    }
   ],
   "source": [
    "t_blob = TextBlob(sentence)\n",
    "lemmatized_sentence = \" \".join([w.lemmatize() for w in t_blob.words])\n",
    "print(lemmatized_sentence)\n",
    "#> the bat saw the cat with stripe hanging upside down by their foot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
